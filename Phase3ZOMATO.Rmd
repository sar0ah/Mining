---
title: "ZOMATO"
output: html_notebook
---

# 1-Problem:

in our project we chose the dataset of the restaurants in Zomato which is an Indian multinational restaurant aggregator and food delivery company. This dataset contains information about various restaurants listed on the platform, including their location, cuisine type, ratings, and other essential attributes.we will study and analyze the restaurants data that will help us identify the preferred cost range and the most likable restaurant type and cuisines type that leads into high ratings to help people who are interested in putting their restaurants on the platform by predicting the ratings of new restaurants on¬†the¬†platform.

2-Data Mining Task:

We want to solve this problem by classification data mining task, to predict the highest ratings based on restaurant type, average cost of two people, and the cuisine's type, by choosing rating as class label.

 3- Data set information:

We choose this data from Kaggle website(<https://www.kaggle.com/datasets/abhijitdahatonde/zomato-restaurants-dataset>) This dataset provides us with information about different type of restaurant and there ratings in food application called Zomato, it consists the following attributes:


##Data set info:
Number of attributes:12

Number of objects:7104

| Attribute         | Description                                                                                                                                   | Type                | Possible values                                               |
|------------------|------------------|-------------------|------------------|
| \#                | Index                                                                                                                                         | Numeric-Ratio       | [0-7014]                                                      |
| Restauran name    | Name of restaurant that application offered by the application                                                                                | Nominal             | #FeelTheROLL #L-81 Cafe #refuel'\@ Biryani Central'\@ The Bbq |
| Restaurant type   | restaurant type includes the type of the restaurant and if it has delivery or takeaway                                                        | Nominal             | Quick Bites ,Casual Dining, Cafe, Delivery Takeaway           |
| Rate              | Express your opinion or assessme                                                                                                              | Numeric- Interval   | [1.8-4.9]                                                     |
| Number of ratings | refers the count of how many times a restaurant or product has been rated or reviewed by customer or user                                     | Numeric- Interval   | [1-16,300]                                                    |
| Avg cost          | Total cost of order                                                                                                                           | Numeric- Interval   | [40-6000]                                                     |
| Online order      | This means that customers have the option to place their food orders through the restaurant's website or a dedicated online ordering platform | Binary (Asymmetric) | 1(Yes),0(No)                                                  |
| Teble booking     | This refers to the process of reserving a table at a restaurant in advance                                                                    | Binary (Asymmetric) | 1(Yes),0(No)                                                  |
| Cuisines type     | This represents the style or category of food and dishes that a restaurant specializes in or offers                                           | Nominal             | North Indian ,Chinese ,North Indian,Fast Food                 |

Type Markdown and LaTeX: ùõº2Type Markdown and LaTeX: ùõº2


Sample of data:


```{r}
print(zomato)
```


Data
```{r}
zomato <- read.csv("zomato.csv")
```

```{r}
summary(zomato)
```

```{r}
head(zomato)
```



```{r}
sum(is.na(zomato))
```




## Boxplot

```{r}
boxplot.stats(zomato$rate)$out

```

```{r}
boxplot.stats(zomato$avgCost)$out

```

```{r}
boxplot.stats(zomato$numofratings)$out

```

#description Rating depend in the reviewer's preferences, these ratings are subjective and can change over time, by this boxplot we can determine the outliers which are few.

```{r}
boxplot(zomato$rate)

```

#### description:

The number of rating boxplot shows that value are close and there hug verity of numbers and a lot of ouliers.

```{r}
boxplot(zomato$numofratings)

```

#### description:

the avrage cost of two people boxpot illustrates that there is few outlairs above 1000

```{r}
boxplot(zomato$avgCost)

```

#### description:

scatterplot visualiz the relationship between two variables and identifying patterns or trends in the data, in this scatterplot we can see that

```{r}
plot(zomato$rate,zomato$avgCost)

```
##This histogram show the frequency average cost, by looking we can se that most people spends¬†between¬†0-1000
```{r}
hist(zomato$avgCost)
```
This pie chart shows if the restaurants offer online order or not, looking at it we can see that they are slightly similar.
```{r}
onlineOrder <- table(zomato$online_order)
pie(onlineOrder)
```

## Statistical Measures
The statistical measures in R, such as mean, standard deviation, median, and others, are primarily applied to numeric data. These measures help in summarizing, describing, and understanding the properties and characteristics of numerical data. When applied to numeric data, they provide valuable insights and descriptive statistics that aid in understanding the distribution, central tendency, variability, and other important characteristics of the dataset.

Numeric statistical measures in R are specifically designed to work with numerical data to derive insights and statistical summaries.
```{r}
summary (zomato$rate)

```

```{r}
summary(zomato$numofratings)

```

```{r}
summary(zomato$avgCost)

```
Data Preprocessing:

Check for missing value:

```{r}
dim(zomato)

```

```{r}
zomato =na.omit(zomato)

```

```{r}
dim(zomato)

```

```{r}
sum(is.na(zomato))

```

### Description:

Missing or null values can significantly impact the effectiveness of a dataset and the quality of insights that can be derived from it. Therefore, we conducted an examination of our data to identify and eliminate any rows containing missing or null values. This process was undertaken to enhance the overall efficiency and reliability of our dataset, ensuring more accurate analysis and valuable information extraction in subsequent¬†steps.

## Detecting and removing the outliers

### Detecting the outliers:

Initially, we detected any unusual data points in the numerical attributes. Subsequently, we removed the rows containing these outliers in order to create a more precise dataset, which would ultimately enhance the accuracy of our later results.

```{r}
library(outliers)
```

#Detect outliers in the 'numofratings' column of the 'zomato' dataset.

```{r}
OutN = outlier(zomato$numofratings, logical =TRUE)

```

Sum the number of outliers detected.

```{r}
sum(OutN)

```

Find the indices of the outliers in the 'numofratings' column.

```{r}
Find_outlier = which(OutN ==TRUE, arr.ind = TRUE)

```

Display the logical vector indicating outliers.

```{r}
OutN
```

Display the indices of the outliers.

```{r}
Find_outlier

```

#### Remove rows with outliers from the 'zomato' dataset.

```{r}
zomato= zomato[-Find_outlier,]

```

We replicated the procedures outlined in the previous code but with the column avgCost.

```{r}
OutAv = outlier(zomato$avgCost, logical =TRUE)
sum(OutAv)

```

```{r}
Find_outlier = which(OutAv ==TRUE, arr.ind = TRUE)
OutAv

```

```{r}
Find_outlier
```

```{r}
zomato= zomato[-Find_outlier,]
```

```{r}
head(zomato)

```
##Encoding categorical data:
Encoding plays a crucial role in various data mining and machine learning tasks, as it facilitates the transformation of raw data into a format that algorithms can effectively process and analyze. This process involves converting categorical or textual data into numerical representations, ensuring compatibility with the computational requirements of the algorithms involved.

##Normalize Data using Min-Max Scaling:

normalization was performed to ensure consistent scaling of the data. The normalization technique applied was the max-min normalization. This technique rescales the values of specific attributes within a defined range between 0 and 1. The following attributes were selected for normalization: numofrating, avgCost We can use the normalized dataset provides a more uniform and comparable representation of the attributes, enabling accurate analysis and modeling for rate.
```{r}
##Encoding:
zomato$tablebooking=factor(zomato$tablebooking , levels =c("No","Yes"), labels = c(0,1))
zomato$online_order=factor(zomato$online_order , levels =c("No","Yes"), labels = c(0,1))
##Normalization:
normalize <- function(x){return ((x-min(x))/(max(x)-min(x)))}

 
```


```{r}
##Normalization:
zomato$numofratings<- normalize(zomato$numofratings)
zomato$avgCost<- normalize(zomato$avgCost)

##Encoding:
zomato$rate <- ifelse(zomato$rate <= 2, "Bad",
                  ifelse(zomato$rate <=3, "Okay",
                      ifelse(zomato$rate <=4, "Good",
                             ifelse(zomato$rate <=5, "Great",0 ))))


```

```{r}
head(zomato)

```

## Features selection:

Features selection: Recursive Feature Elimination using Package Randomforst is the feature selection method we'll use to streamline our predictive model. This method is commonly used to determine which input variables are most important for predicting our focal variable, in this case, the "rate." In addition, the varImp function will be utilized to assess the importance of different variables in our investigation. 

```{r}
# load the library
 library(mlbench)
library(caret)
library(randomForest)
```



```{r}
# ensure the results are repeatable
set.seed(7)

# Convert the class label to a factor

zomato$rate <- as.factor(zomato$rate )



# Separate the predictors and the class label

predictors <- zomato[, -5]  # Excluding the class label (satisfaction)
class_label <- zomato$rate 


# Train a Random Forest model

model <- randomForest(predictors, class_label, importance = TRUE)



# Get the variable importance

importance <- importance(model)

ranked_features <- sort(importance[, "MeanDecreaseGini"], decreasing = TRUE)



# Print the ranked features
print(ranked_features)

barplot(ranked_features, horiz = TRUE, col = c("lightblue2"), las = 1, main = "Features selection")
```






##balance:

Balance or imbalance ?The provided code snippet in R performs a process known as data upscaling or oversampling. This technique is commonly used to address imbalances within categorical classes by generating additional instances of the underrepresented class.



Zomato data set is Balance 




```{r}
library(ROSE)
# upscaling the data

zomato$rate <- as.factor(zomato$rate)
zomato<-upSample(zomato[,-5],zomato$rate, yname="rate")
plot(zomato$rate)

# checking the number of stroke/ non-stroke observations
prop.table(table(zomato$rate))
title(main="Data after oversampling", xlab="rate", ylab="observations")

 

```

Classification:
  We employed both supervised and unsupervised learning methodologies on our dataset by leveraging classification and clustering techniques.
  For classification, we utilized a decision tree‚Äîa recursive algorithm generating a tree structure with leaf nodes that represent final decisions.
  Our model is geared towards predicting the class label ('rate'), categorized into four classes: 'Good,' 'Great,' and 'Bad 'Okaya'.
  The prediction is based on the remaining attributes ('numRatin' and 'avgCost').
 This technique involved the following steps:

 Data Splitting:
   - Dividing the dataset into two distinct sets:
      - Training dataset: Employed for constructing the decision tree.
       - Testing dataset: Utilized to assess and validate the built model.

 Model Evaluation:
  - Measurement of the model's performance:
      - Accuracy and  Measures: Evaluated using a confusion matrix,
         providing insights into the model's predictive accuracy and its performance regarding the different classes.
 The tools and packages used in this process involved the 'party' and 'caret' packages for classification tasks. Specifically:

 Methodologies and Techniques:
    - Data Splitting: Employed the 'sample' method to partition the dataset into training and testing subsets.
#   - Decision Tree Construction: Leveraged the 'rpart.plot' method for building the decision tree structure.
#   - Prediction and Testing: Utilized the 'predict' method to assess and test the model's predictions.
#   - Model Evaluation: Employed the 'confusionMatrix' method to assess the model's performance through the confusion matrix.

 The following steps were engaged in our supervised classification model's training procedure.
 Since classification is a type of supervised learning, the model needs to be trained using training data.
We carried out our training protocol in the following ways:
Data Splitting for Training:

 - Utilization of the 'rpart()' method to partition the dataset into two distinct subsets: training data and testing data.
  - We experimented with three different sizes for the training subset: 60%, 75%, and 80%.
     This variation aimed to identify the subset size that yielded the highest accuracy for our model.
   - Considering the limited size of our dataset, we set the 'replace' attribute to "TRUE."
   This setting allowed the selection of tuples in the training data with replacement,
     while the remaining data was included in the testing data portion.
  - Emphasized allocating the largest proportion to the training subset.
    This decision was made due to the belief that a larger training set improves our model's predictive capabilities.



```{r}
# Make sure you have the required libraries installed


# Load necessary libraries
library(rpart)
library(rpart.plot)



```


Information gain is a concept used in decision tree algorithms to choose the best attributes for splitting data.
  It quantifies how much a feature reduces uncertainty or disorder in the dataset .
 Features with higher information gain are preferred as they provide more helpful insights when making decisions in the tree, improving the accuracy of the model,use tow package ('rpat','rpart.plot').



```{r}

#partioning the data into (60 % training,40% testing)
  
  
set.seed(123)
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.60, 0.40))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]

# Assuming zomato_ctree is generated using rpart
# Example of fitting an rpart model
zomato_ctree <- rpart(rate ~ numofratings + avgCost, data=trainData,parm=list(split="information"))

# Visualize the decision tree using rpart.plot
rpart.plot(zomato_ctree)



```
 Descriptin:
The first node represents the rate 'Good'. This means that the initial splitting of the data based on the 'rate' feature led to a primary node labeled as 'Good'. It branches to 'Bad' when  and 'Okay' when 'numofrating' is larger than 0.014 This indicates that the decision tree further classifies 'Good' into 'Bad' and 'Okay' based on conditions related to the 'numofrating' feature. The leaf nodes represent all class labels 'Bad', 'Great', 'Okay'. Each leaf node represents a final decision or label. The 'Great' label is repeated more than others, with the average cost for 'Bad' and 'Great' being less than 0.31. For 'Great', the percentage is >=0.15 and for 'Good', it's less. This section seems to detail certain conditions within the branches or nodes. It suggests that 'Great' is more frequent, with specific criteria related to 'avg cost' and 'percentage'. For 'Good', 'Great', and 'Okay', 'numofrating' varies accordingly. This part of the interpretation explains the 'numofrating' conditions for 'Good', 'Great', and 'Okay'.

```{r}
# Make predictions on the test set
predictions <- predict(zomato_ctree, testData, type = "class")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)


```


```{r}

#partioning the data into (75 % training,25% testing)
  
  
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.75, 0.25))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]

# Assuming zomato_ctree is generated using rpart
# Example of fitting an rpart model
zomato_ctree <- rpart(rate ~ numofratings + avgCost, data = trainData,parm=list(split="information"))

# Visualize the decision tree using rpart.plot
rpart.plot(zomato_ctree)



```
Discription:The first node represents the rate 'Bad'. This means that the initial splitting of the data based on the 'rate' feature led to a primary node labeled as 'Great'. It branches to 'Bad'  and 'Okay' when 'numofrating' is larger than 0.014,This indicates that the decision tree further classifies 'Great' into 'Bad' and 'Okay' based on conditions related to the 'numofrating' feature. When the rate is 'okay,'(52%) the numofrating is less than 0.024 Good' and avgCost >= 0.15 for 'Bad' (46%) numofrating is less than 0.026. Each leaf node represents a final decision or label, where 'Bad' has (92, 27%) instances with avgCost < 0.31, 'Great' has (0.5%) instances with avgCost < 0.31, and 'Good' has (0.0,14%)and 'Good'(00,18%) instances with numofrating >= 0.026,then 'Good' (00,20%) numofrating<0.024,it's close to dession tree (60 % training,40% testing) the difference between them is simple.

```{r}
# Make predictions on the test set
predictions <- predict(zomato_ctree, testData, type = "class")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)


```


```{r}

#partioning the data into (80 % training,20% testing)
  
  
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.80, 0.20))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]

# Assuming zomato_ctree is generated using rpart
# Example of fitting an rpart model
zomato_ctree <- rpart(rate ~ numofratings + avgCost, data = trainData,parm=list(split="information"))

# Visualize the decision tree using rpart.plot
rpart.plot(zomato_ctree)

```
The first node represents the ‚Äúgreat‚Äù rate. This means that the initial partitioning of the data based on the ‚Äúaverage‚Äù feature resulted in a primary node marked as ‚Äúgreat‚Äù. It is divided into ‚Äúbad‚Äù  and ‚Äúokay‚Äù when the ‚Äúnumber of rating ‚Äù is greater than 0014. This indicates that the decision tree classifies ‚Äúgood‚Äù into ‚Äúbad‚Äù and ‚Äúgood‚Äù based on conditions related to the ‚Äúnumber‚Äù feature. The leaf nodes represent all the category labels ‚Äúbad,‚Äù ‚Äúgreat,‚Äù and ‚Äúgood.‚Äù Each leaf node represents a final decision or designation. The rating "bad" and "Good" and "Okay, then Great' is more frequent than others, with the average cost of ‚ÄúBad‚Äù and ‚ÄúGreat‚Äù less than 0.31. For ‚ÄúGreat‚Äù(15%)the ratio is greater than = 0.15 and for ‚ÄúGood‚Äù, it is less. This section appears to detail certain conditions within branches or nodes. She notes that ‚Äúgreat‚Äù is the most frequently mentioned word, with specific criteria being ‚Äúaverage cost‚Äù and ‚Äúpercentage.‚Äù For 'good', 'great' and 'okay', the 'number' varies accordingly, is great accuracy than other tree,i think it's butter.



```{r}
# Make predictions on the test set
predictions <- predict(zomato_ctree, testData, type = "class")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)



```



```{r}
library(ipred)
library(rpart)
library(rpart.plot)
library(caTools)
library(party)

```


#Gain Ratio
#Decision trees are powerful tools in machine learning that enable the exploration of complex decision-making processes. Gain Ratio Decision Trees, 
#a variant of traditional decision trees, offer a sophisticated approach to constructing a predictive model by intelligently selecting the most informative features. The Gain Ratio is a criterion used in the process of decision tree induction, specifically designed to overcome certain limitations associated with other splitting criteria, such as Information Gain.
#The primary objective of a Gain Ratio Decision Tree is to create a tree structure that efficiently classifies or predicts outcomes based on the available input features. Unlike Information Gain, which tends to favor attributes with a large number of categories, Gain Ratio introduces a normalization factor that helps balance the bias towards attributes with many categories. This makes Gain Ratio particularly useful when dealing with datasets containing features of varying cardinalities.


```{r}
#partition the data into ( 60% training, 40% testing):
#splitting 60% of data for training, 40% of data for testing
set.seed(123)
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.60, 0.40))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]



myFormula<- rate~ avgCost+numofratings
rate_ctree <- ctree(myFormula, data = trainData, controls = ctree_control(maxdepth = 3))
# check the prediction
table(predict(rate_ctree), trainData$rate)
print(rate_ctree)

# Plot the ctree
plot(rate_ctree, type = "simple")
plot(rate_ctree, type = "simple", extra = 1, under = TRUE)






```


```{r}
predictions <- predict(rate_ctree, testData, type = "response")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```


```{r}
#partition the data into ( 75% training, 25% testing):
#splitting 75% of data for training, 25% of data for testing
set.seed(123)
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.75, 0.25))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]

myFormula<- rate~ avgCost+numofratings
rate_ctree <- ctree(myFormula, data = trainData, controls = ctree_control(maxdepth = 3))
# check the prediction
table(predict(rate_ctree), trainData$rate)
print(rate_ctree)
plot(rate_ctree,type="simple")
plot(rate_ctree,type = "simple", extra = 1, under = TRUE)







```


```{r}
 predictions <- predict(rate_ctree, testData, type = "response")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)

```


```{r}
#partition the data into ( 80% training, 20% testing):
#splitting 80% of data for training, 20% of data for testing
set.seed(123)
ind <- sample(2, nrow(zomato), replace=TRUE, prob=c(0.80, 0.20))
trainData <- zomato[ind==1,]
testData <- zomato[ind==2,]


myFormula<- rate~ avgCost+numofratings
rate_ctree <- ctree(myFormula, data = trainData, controls = ctree_control(maxdepth = 3))
# check the prediction
table(predict(rate_ctree), trainData$rate)
print(rate_ctree)
plot(rate_ctree,type="simple")
plot(rate_ctree,type = "simple", extra = 1, under = TRUE)





```


```{r}
# predict on test data
predictions <- predict(rate_ctree, testData, type = "response")  
 
#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

install.packages('caret')
library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```





In the realm of decision tree algorithms and binary classification problems, the Gini Index stands as a fundamental metric for evaluating the impurity or disorder within a dataset. this index serves as a criterion for determining the optimal split of data points, facilitating the construction of decision trees that effectively classify and predict outcomes.
The Gini Index quantifies the probability of incorrectly classifying a randomly chosen element in the dataset. In the context of decision trees, it measures the impurity of a particular node by assessing the likelihood that a randomly selected sample from the node is misclassified. A lower Gini Index implies higher purity, indicating a more homogeneous set of samples in terms of the target variable.
(We tried all possible codes from multiple resources and it doesn‚Äôt show an error message but still there is no output)

partition the data into ( 60% training, 40% testing):


```{r}

set.seed(123)

split = sample.split(zomato$rate, SplitRatio = 0.60)

training_set = subset(zomato, split == TRUE)
test_set = subset(zomato, split == FALSE)
tree <- rpart(rate ~ ., data = training_set,method = 'class')
plot(tree)
rpart.plot(tree)
print(plot(tree))
print(rpart.plot(tree))



# Make predictions on the test set
predictions <- predict(tree, testData, type = "response")  

#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)


```

partition the data into ( 75% training, 25% testing):

```{r}
#splitting 75% of data for training, 25% of data for testing


split = sample.split(zomato$rate, SplitRatio = 0.75)

training_set = subset(zomato, split == TRUE)
test_set = subset(zomato, split == FALSE)
tree <- rpart(rate ~ ., data = training_set,method = 'class')
plot(tree)
rpart.plot(tree)
print(plot(tree))
print(rpart.plot(tree))
```
```{r}

# Make predictions on the test set
predictions <- predict(tree, testData, type = "response")  

#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

partition the data into ( 80% training, 20% testing):


```{r}
#splitting 80% of data for training, 20% of data for testing

split = sample.split(zomato$rate, SplitRatio = 0.80)

training_set = subset(zomato, split == TRUE)
test_set = subset(zomato, split == FALSE)
tree <- rpart(rate ~ ., data = training_set,method = 'class')
plot(tree)
rpart.plot(tree)
print(plot(tree))
print(rpart.plot(tree))
```
```{r}

# Make predictions on the test set
predictions <- predict(tree, testData, type = "response")  

#Accuracy of model is given by
((10+12+14)/nrow(testData))*100

library(caret)
results <- confusionMatrix(predictions, testData$rate)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```


```{r}
```







## Clustring
#Description: Clustering dataset is useful to group similar data together, and we have decided to cluster the dataset into 3,5, 7 clusters based on some evaluation method such as elbow method by (wssplot) function, and silhouette coefficient which helped us to figure how many clusters are needed for zomato dataset. The result was¬†precisely¬†3.
#### numeric data and scaling

```{r}
numeric_data <- zomato[sapply(zomato, is.numeric)]
colMeans(numeric_data, na.rm = TRUE)

numeric_data <- scale(numeric_data)
```

```{r}
set.seed(8953)
library(ggplot2)
library(factoextra)


```

```{r}
k3<- kmeans(numeric_data,centers = 3, nstart = 20)
str(k3)
fviz_cluster(k3,data = numeric_data)

```

```{r}
k5<- kmeans(numeric_data,centers = 5, nstart = 20)
str(k5)
fviz_cluster(k5,data = numeric_data)
```

```{r}
k8<- kmeans(numeric_data,centers = 8, nstart = 20)
str(k8)
fviz_cluster(k8,data = numeric_data)

```

```{r}
##elbow
wssplot<- function (data, nc=15 ,seed=1234)
{
  wss<-(nrow(data)-1)*sum(apply(data,2,var))
  for(i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data,centers = i)$withinss)}
  plot(1:nc,wss, type="b", xlab="number of clusters", ylab="within groups sum of squares")
}

wssplot(numeric_data)
```

```{r}
fviz_nbclust(numeric_data, # data
             kmeans, # clustering algorithm
             method = "silhouette") # silhouette
```

```{r}
fviz_nbclust(numeric_data, # data  
             kmeans, # clustering algorithm 
             nstart = 25,# if centers is a number, how many random sets should be chosen?(default is 25)
             iter.max = 200, # the maximum number of iterations allowed.
             method = "wss" ) # elbow method


```




```{r}

cluster_assignments <- c(k5$cluster)
ground_truth_labels <- c(zomato$rate)
data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)


# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
    # Count the number of items from the same category within the same cluster
    same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
    # Count the total number of items in the same cluster
    total_same_cluster <- sum(data$cluster == cluster)
    
    # Count the total number of items with the same category
    total_same_category <- sum(data$label == label)
    
    # Calculate precision and recall for the current item and add them to the sums
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")


```


